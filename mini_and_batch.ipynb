{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXtjWz_yF5j6",
    "outputId": "7dd001b8-6266-4725-8bb9-cede850d1513"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2BZoEjAGTN7",
    "outputId": "bdd6d71c-09a4-4d85-9ace-6af2ab621e8b"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/2-1.zip\" -d \"/content/Task-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyORvKfjGyoR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW34L4ipHD9d"
   },
   "outputs": [],
   "source": [
    "def adding_label(df):\n",
    "    unique_symbols = sorted(df[\"symbol_id\"].unique())  # Sorted unique symbol_ids\n",
    "    symbol_to_index = {symbol: i for i, symbol in enumerate(unique_symbols)}\n",
    "    return symbol_to_index\n",
    "\n",
    "def one_hot_encode(label, num_classes):\n",
    "    one_hot = torch.zeros(num_classes)  # Create zero vector of shape (369,)\n",
    "    one_hot[label] = 1  # Set the correct index to 1\n",
    "    return one_hot\n",
    "\n",
    "def load_and_flatten_image(img_path, target_size=(32, 32)):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale (H, W)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read {img_path}\")\n",
    "        return torch.zeros(target_size[0] * target_size[1], dtype=torch.float32)  # Empty tensor for missing files\n",
    "\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "    img = torch.tensor(img)  # Convert to PyTorch tensor\n",
    "\n",
    "    return img.flatten()  # Convert to 1D tensor (1024,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihm2TNKFHGR0",
    "outputId": "b076ee2d-e12b-4c47-9e95-3d81eddf477a"
   },
   "outputs": [],
   "source": [
    "load_and_flatten_image(\"/content/Task-1/2-1/images/v2-00016.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4_D6qOXHIsv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, learning_rate, activation_function, device=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation_function\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _Initialize(self, input_size, output_size):\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            self.weights.append(torch.randn(hidden_size, prev_size, dtype=torch.float32, device=self.device) * 0.01)\n",
    "            self.bias.append(torch.randn(hidden_size, 1, dtype=torch.float32, device=self.device) * 0.01)\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        self.weights.append(torch.randn(output_size, prev_size, dtype=torch.float32, device=self.device) * 0.01)\n",
    "        self.bias.append(torch.randn(output_size, 1, dtype=torch.float32, device=self.device) * 0.01)\n",
    "\n",
    "    def _Activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.relu(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "\n",
    "    def _ActivationPrime(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return torch.where(x > 0, torch.tensor(1.0, dtype=x.dtype, device=self.device), torch.tensor(0.0, dtype=x.dtype, device=self.device))\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sig = torch.sigmoid(x)\n",
    "            return sig * (1 - sig)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - torch.tanh(x) ** 2\n",
    "        elif self.activation == 'linear':\n",
    "            return torch.ones_like(x, device=self.device)\n",
    "\n",
    "    def _Softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
    "        return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
    "\n",
    "    def _CrossEntropy(self, y, y_hat):\n",
    "        return -torch.mean(torch.sum(y * torch.log(y_hat + 1e-9), dim=1))\n",
    "\n",
    "    def _Forward(self, X):\n",
    "        X = X.to(self.device)\n",
    "        activations = X\n",
    "        self.layer_inputs = []\n",
    "        self.z = []\n",
    "\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = torch.mm(activations, self.weights[i].T) + self.bias[i].T\n",
    "            activations = self._Activation(z)\n",
    "            self.layer_inputs.append(activations)\n",
    "            self.z.append(z)\n",
    "\n",
    "        z = torch.mm(activations, self.weights[-1].T) + self.bias[-1].T\n",
    "        self.layer_inputs.append(z)\n",
    "        return self._Softmax(z)\n",
    "\n",
    "    def _Backward(self, X, y, y_hat):\n",
    "        batch_size = X.shape[0]\n",
    "        dz = (y_hat - y) / batch_size\n",
    "        grads_w, grads_b = [], []\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            dw = torch.mm(dz.T, self.layer_inputs[i - 1] if i > 0 else X)\n",
    "            db = torch.sum(dz, dim=0, keepdim=True).T\n",
    "\n",
    "            if i > 0:\n",
    "                dz = torch.mm(dz, self.weights[i]) * self._ActivationPrime(self.z[i - 1])\n",
    "\n",
    "            grads_w.insert(0, dw)\n",
    "            grads_b.insert(0, db)\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.bias[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "    def batch_fit(self, X, y, epochs):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self._Forward(X)\n",
    "            self._Backward(X, y, y_hat)\n",
    "\n",
    "            loss = self._CrossEntropy(y, y_hat).item()\n",
    "            acc = self.accuracy(y_hat, y)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        return losses, accuracies\n",
    "\n",
    "    def Mini_batch_fit(self, X, y, epochs, batch_size):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "\n",
    "            y_pred = self._Forward(X)\n",
    "            loss = self._CrossEntropy(y, y_pred).item()\n",
    "            acc = self.accuracy(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        return losses, accuracies\n",
    "\n",
    "    def SGD_fit(self, X, y, epochs):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "        self._Initialize(X.shape[1], y.shape[1])\n",
    "        losses, accuracies = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                y_hat = self._Forward(X_batch)\n",
    "                self._Backward(X_batch, y_batch, y_hat)\n",
    "\n",
    "            y_pred = self._Forward(X)\n",
    "            loss = self._CrossEntropy(y, y_pred).item()\n",
    "            acc = self.accuracy(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        return losses, accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.to(self.device)\n",
    "        return self._Forward(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        y_hat = self._Forward(X)\n",
    "        return self._CrossEntropy(y, y_hat)\n",
    "\n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        y_pred, y_true = y_pred.to(self.device), y_true.to(self.device)\n",
    "        predicted_classes = torch.argmax(y_pred, dim=1)\n",
    "        actual_classes = torch.argmax(y_true, dim=1)\n",
    "        return (predicted_classes == actual_classes).float().mean().item() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPQtuH_cHS2q",
    "outputId": "e0c26c5f-cc24-417e-9e03-417510441ccf"
   },
   "outputs": [],
   "source": [
    "# Map symbol_id to labels\n",
    "symbols = pd.read_csv(\"/content/Task-1/2-1/Symbols/symbols.csv\")\n",
    "unique_symbols = sorted(symbols[\"symbol_id\"].unique())\n",
    "symbol_to_index = {symbol: i for i, symbol in enumerate(unique_symbols)}\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def process_fold(fold_num):\n",
    "    \"\"\"Loads data for a given fold and preprocesses it.\"\"\"\n",
    "    train = pd.read_csv(f\"/content/Task-1/2-1/Symbols/classification-task/fold-{fold_num}/train.csv\")\n",
    "    test = pd.read_csv(f\"/content/Task-1/2-1/Symbols/classification-task/fold-{fold_num}/test.csv\")\n",
    "\n",
    "    train[\"path\"] = train[\"path\"].str.replace(\"../../images/\", \"/content/Task-1/2-1/images/\", regex=False)\n",
    "    test[\"path\"] = test[\"path\"].str.replace(\"../../images/\", \"/content/Task-1/2-1/images/\", regex=False)\n",
    "\n",
    "    train[\"label\"] = train[\"symbol_id\"].map(symbol_to_index)\n",
    "    test[\"label\"] = test[\"symbol_id\"].map(symbol_to_index)\n",
    "\n",
    "    num_classes = len(unique_symbols)\n",
    "    y_train = torch.stack([one_hot_encode(label, num_classes) for label in train[\"label\"]]).to(device)\n",
    "    y_test = torch.stack([one_hot_encode(label, num_classes) for label in test[\"label\"]]).to(device)\n",
    "\n",
    "    X_train = torch.stack([load_and_flatten_image(path) for path in train[\"path\"]]).to(device)\n",
    "    X_test = torch.stack([load_and_flatten_image(path) for path in test[\"path\"]]).to(device)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nWa5iCUgHKMv",
    "outputId": "2e7e8be7-bfd1-4c0a-92b5-b87825e3e9d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define hyperparameters to test\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "optimizers = ['batch_gd', 'mini_batch_gd']\n",
    "\n",
    "# Store results per (activation, optimizer) combination\n",
    "results = { (activation, optimizer): {'errors': [], 'accuracies': [], 'loss_history': [], 'acc_history': []}\n",
    "            for activation in activations for optimizer in optimizers }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over folds first\n",
    "for fold in range(1, 11):  # 10 folds\n",
    "    print(f\"\\nProcessing Fold {fold}...\")\n",
    "    X_train, y_train, X_test, y_test = process_fold(fold)\n",
    "\n",
    "    for activation in activations:\n",
    "        for optimizer in optimizers:\n",
    "            print(f\"Now training with Activation: {activation}, Optimizer: {optimizer} on Fold {fold}...\")\n",
    "\n",
    "            # Initialize model and move it to GPU\n",
    "            mlp = MLP(input_size=1024, output_size=369, hidden_sizes=[1024, 512],\n",
    "                      learning_rate=0.05, activation_function=activation)\n",
    "\n",
    "            # Train using the selected optimizer\n",
    "            if optimizer == \"batch_gd\":\n",
    "                loss_history, acc_history = mlp.batch_fit(X_train, y_train, epochs=10)\n",
    "            elif optimizer == \"mini_batch_gd\":\n",
    "                loss_history, acc_history = mlp.Mini_batch_fit(X_train, y_train, epochs=10, batch_size=50)\n",
    "            elif optimizer == \"sgd\":\n",
    "                loss_history, acc_history = mlp.SGD_fit(X_train, y_train, epochs=3)\n",
    "            else:\n",
    "                print(\"No such optimizer found!\")\n",
    "                continue\n",
    "\n",
    "            # Evaluate\n",
    "            error = mlp.evaluate(X_test, y_test).item()\n",
    "            y_pred = mlp.predict(X_test)\n",
    "            accuracy = mlp.accuracy(y_test, y_pred)\n",
    "\n",
    "            print(f\"Error: {error}\")\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "            # Store results\n",
    "            results[(activation, optimizer)]['errors'].append(error)\n",
    "            results[(activation, optimizer)]['accuracies'].append(accuracy)\n",
    "            results[(activation, optimizer)]['loss_history'].append(loss_history)\n",
    "            results[(activation, optimizer)]['acc_history'].append(acc_history)\n",
    "\n",
    "# Compute final averages and identify best combinations\n",
    "best_accuracy = -1\n",
    "best_accuracy_combo = None\n",
    "best_error = float('inf')\n",
    "best_error_combo = None\n",
    "\n",
    "print(\"\\nFinal Results Across All Folds:\")\n",
    "for (activation, optimizer), data in results.items():\n",
    "    mean_error = np.mean(data['errors'])\n",
    "    std_error = np.std(data['errors'])\n",
    "    mean_accuracy = np.mean(data['accuracies'])\n",
    "    std_accuracy = np.std(data['accuracies'])\n",
    "\n",
    "    print(f\"Activation: {activation}, Optimizer: {optimizer}, \"\n",
    "          f\"Mean Error: {mean_error:.4f} Â± {std_error:.4f}, \"\n",
    "          f\"Mean Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
    "\n",
    "    # Update best accuracy combination\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_accuracy_combo = (activation, optimizer)\n",
    "\n",
    "    # Update best error combination\n",
    "    if mean_error < best_error:\n",
    "        best_error = mean_error\n",
    "        best_error_combo = (activation, optimizer)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n=== Best Combinations ===\")\n",
    "print(f\"ðŸ† Best Accuracy: {best_accuracy:.4f} using Activation: {best_accuracy_combo[0]}, Optimizer: {best_accuracy_combo[1]}\")\n",
    "print(f\"ðŸŽ¯ Best (Lowest) Error: {best_error:.4f} using Activation: {best_error_combo[0]}, Optimizer: {best_error_combo[1]}\")\n",
    "\n",
    "print(f\"\\nTotal Execution Time: {(end_time - start_time):.2f} seconds\")\n",
    "\n",
    "# Plot for best accuracy combination\n",
    "best_loss_history = np.mean(results[best_accuracy_combo]['loss_history'], axis=0)\n",
    "best_acc_history = np.mean(results[best_accuracy_combo]['acc_history'], axis=0)\n",
    "\n",
    "epochs = range(1, len(best_loss_history) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, best_loss_history, label='Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss ({best_accuracy_combo[0]} + {best_accuracy_combo[1]})')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, best_acc_history, label='Accuracy', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Training Accuracy ({best_accuracy_combo[0]} + {best_accuracy_combo[1]})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdsirpbZm4ql"
   },
   "source": [
    "### Model Performance and Consistency: Mean and Standard Deviation\n",
    "\n",
    "#### 1. What do the mean and standard deviation tell you about model performance and consistency?\n",
    "- **Mean Accuracy**: Represents the average performance of the model across the 10 folds. A higher mean accuracy generally indicates better performance.\n",
    "- **Standard Deviation (Std)**: Measures how much the accuracy varies across different folds. A lower standard deviation suggests that the model is consistently performing well, while a higher standard deviation indicates instability or sensitivity to the training data.\n",
    "\n",
    "#### 2. How does a high vs. low standard deviation impact confidence in the modelâ€™s generalization?\n",
    "- **High Standard Deviation**: Indicates that the model's performance varies significantly across different folds, which suggests that the model may not generalize well to unseen data.\n",
    "- **Low Standard Deviation**: Suggests that the model's performance is stable and reliable across different subsets of the dataset, increasing confidence in its ability to generalize well to new data.\n",
    "\n",
    "#### 3. Comparing two configurations: One with slightly higher mean accuracy but significantly higher standard deviation vs. one with marginally lower mean accuracy but lower standard deviation.\n",
    "- Model with a **higher mean accuracy** has a significantly **higher standard deviation**, it may be less reliable and more sensitive to data variations.\n",
    "- Model with a **slightly lower mean accuracy** has a **much lower standard deviation**, it is likely to generalize better since it exhibits more stable performance.\n",
    "- **Preferred Choice**: Model with the **lower standard deviation** is preferable, as it is more robust and reliable, even if it has a slightly lower mean accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
